{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7258112",
   "metadata": {},
   "source": [
    "# Generate Image Embeddings for the Duke-WLOA-AMD Dataset\n",
    "\n",
    "- The original dataset is in .mat format, however, for each of access we converted the images and annotations into png and tried extracting embeddings. How3ever, the conversion to .png might have introduced noise or artifacts and thus its not workings.\n",
    "- Here i am trying to first preprocess the .mat data files and then extract embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ccf6a1",
   "metadata": {},
   "source": [
    "### Table of Content: <a id = 'table_of_contents'></a>\n",
    "0. [imports](#imports)\n",
    "1. [Data Loading](#dataload)\n",
    "2. [Data Preprocessing & Transform](#dataprocess)\n",
    "3. [Load Model and Extract Embedding](#load)\n",
    "4. [Save Embeddings](#save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa84921",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3771650",
   "metadata": {},
   "source": [
    "## 0. Imports <a id ='imports'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cc32e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suraj/anaconda3/envs/vision-2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "import scipy as sp \n",
    "import scipy.io as sio \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel, ViTImageProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading <a id = 'dataload'></a> \n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075ba1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a4b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir containing the .mat files\n",
    "data_dir = '/home/suraj/Data/Duke_WLOA_RL_Annotated/AMD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f346d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SESSION SETUP\n",
    "data_files  =  [os.path.join(data_dir, f) for f in os.listdir(data_dir) if os.path.isfile(os.path.join(data_dir, f))] # List of data files to process\n",
    "output_dir = os.path.join(data_dir,'embeddings')  # Directory to save the output files\n",
    "if output_dir and not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ebe3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ MULTIPLE .MAT FILES on after another\n",
    "#  Read the .mat file in scipt \n",
    "def read_mat_file(file_path)-> dict:\n",
    "    ''' \n",
    "    Reads a .mat file and returns the data as a dictionary.\n",
    "    Args:\n",
    "        file_path (str): Path to the .mat file.\n",
    "    Returns:\n",
    "        dict: Data contained in the .mat file.\n",
    "    '''\n",
    "    try:\n",
    "        data = sp.io.loadmat(file_path, squeeze_me=True)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89d20513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each .mat file\n",
    "for file_path in data_files:\n",
    "    data = read_mat_file(file_path)\n",
    "    if data is None:\n",
    "        continue #skip this file if it could not be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dc496db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100eb08",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing & Transformation <a id = 'dataprocess'> </a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# reading the file [x] in data_files[]\n",
    "subject_dct = read_mat_file(data_files[0])  # Replace 0 with the index of the file you want to read\n",
    "images = subject_dct[\"images\"]  # \n",
    "layer_maps = subject_dct[\"layerMaps\"]  # Shape: (100, 1000, 3); # 100 is the number of b-scans, 1000 is the width, and 3 is the number of layers (ILM, RPE, and BR)\n",
    "subject_dct.keys()\n",
    "'''\n",
    "# Preprocess each b-scan in one .mat file\n",
    "images = data.get('images')\n",
    "layer_maps = data.get('layerMaps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "238a8059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (512, 1000, 100)\n",
      "Layer maps shape: (100, 1000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Layer maps shape:\", layer_maps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess each b-scan in one .mat file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fbdcf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=Image.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e28048ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process each b-scan and convert to RGB, apply transformations, and stack them into a tensor\n",
    "b_scans = []\n",
    "for i in range(images.shape[2]):\n",
    "    b_scan = images[:, :, i]\n",
    "    b_scan_rgb = np.stack([b_scan] * 3, axis=-1)\n",
    "    b_scan_rgb = (b_scan_rgb / b_scan_rgb.max() * 255).astype(np.uint8)\n",
    "    img = Image.fromarray(b_scan_rgb)\n",
    "    b_scans.append(transform(img))\n",
    "b_scans = torch.stack(b_scans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88db71",
   "metadata": {},
   "source": [
    "## 3. Load Model and Extract Embeddings <a id = 'load'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7c0d64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTModel(\n",
       "  (embeddings): ViTEmbeddings(\n",
       "    (patch_embeddings): ViTPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ViTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ViTLayer(\n",
       "        (attention): ViTSdpaAttention(\n",
       "          (attention): ViTSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (pooler): ViTPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Load Model\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a860f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Extract Embeddings\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for b_scan in b_scans:\n",
    "        inputs = b_scan.unsqueeze(0)\n",
    "        outputs = model(pixel_values=inputs)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "        embeddings.append(embedding.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "481b4fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a46c22",
   "metadata": {},
   "source": [
    "## 4. Save Embeddings <a id = 'save'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ddab441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to /home/suraj/Data/Duke_WLOA_RL_Annotated/AMD/embeddings/Farsiu_Ophthalmology_2013_AMD_Subject_1153_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings with a filename based on the input file\n",
    "file_name = os.path.basename(file_path).replace('.mat', '_embeddings.npy')\n",
    "output_path = os.path.join(output_dir, file_name)\n",
    "np.save(output_path, embeddings)\n",
    "print(f\"Saved embeddings to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7cc9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model and preprocessor\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=Image.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# List of .mat files\n",
    "mat_files = [f'path_to_data/patient_{i:03d}.mat' for i in range(1, 201)]  # 200 files\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
