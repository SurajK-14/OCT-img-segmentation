{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7258112",
   "metadata": {},
   "source": [
    "# Generate Image Embeddings for the Duke-WLOA-AMD Dataset\n",
    "\n",
    "- The original dataset is in .mat format, however, for each of access we converted the images and annotations into png and tried extracting embeddings. How3ever, the conversion to .png might have introduced noise or artifacts and thus its not workings.\n",
    "- Here i am trying to first preprocess the .mat data files and then extract embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ccf6a1",
   "metadata": {},
   "source": [
    "### Table of Content: <a id = 'table_of_contents'></a>\n",
    "0. [imports](#imports)\n",
    "1. [Data Loading](#dataload)\n",
    "2. [Data Preprocessing & Transform](#dataprocess)\n",
    "3. [Load Model and Extract Embedding](#load)\n",
    "4. [Save Embeddings](#save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa84921",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3771650",
   "metadata": {},
   "source": [
    "## 0. Imports <a id ='imports'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc32e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suraj/anaconda3/envs/vision-2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "import scipy as sp \n",
    "import scipy.io as sio \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel, ViTImageProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading <a id = 'dataload'></a> \n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075ba1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a4b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir containing the .mat files\n",
    "data_dir = '/home/suraj/Data/Duke_WLOA_RL_Annotated/AMD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f346d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SESSION SETUP\n",
    "data_files  =  [os.path.join(data_dir, f) for f in os.listdir(data_dir) if os.path.isfile(os.path.join(data_dir, f))] # List of data files to process\n",
    "output_dir = os.path.join(data_dir,'embeddings')  # Directory to save the output files\n",
    "if output_dir and not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ebe3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ MULTIPLE .MAT FILES on after another\n",
    "#  Read the .mat file in scipt\n",
    "def read_mat_file(file_path)-> dict:\n",
    "    ''' Reads a .mat file and returns the data as a dictionary.\n",
    "    Args:\n",
    "        file_path (str): Path to the .mat file.\n",
    "    Returns:\n",
    "        dict: Data contained in the .mat file.'''\n",
    "    try:\n",
    "        data = sp.io.loadmat(file_path, squeeze_me=True)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100eb08",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing & Transformation <a id = 'dataprocess'> </a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1a8eef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'images', 'layerMaps', 'Age'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the file [x] in data_files[]\n",
    "subject_dct = read_mat_file(data_files[0])  # Replace 0 with the index of the file you want to read\n",
    "images = subject_dct[\"images\"]  # Shape: (512, 1000, 100); 512 is the height, 1000 is the width, and 100 is the number of b-scans\n",
    "layer_maps = subject_dct[\"layerMaps\"]  # Shape: (100, 1000, 3); # 100 is the number of b-scans, 1000 is the width, and 3 is the number of layers (ILM, RPE, and BR)\n",
    "subject_dct.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "238a8059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (512, 1000, 100)\n",
      "Layer maps shape: (100, 1000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Layer maps shape:\", layer_maps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess each b-scan in one .mat file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fbdcf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=Image.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e28048ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process each b-scan and convert to RGB, apply transformations, and stack them into a tensor\n",
    "b_scans = []\n",
    "for i in range(images.shape[2]):\n",
    "    b_scan = images[:, :, i]\n",
    "    b_scan_rgb = np.stack([b_scan] * 3, axis=-1)\n",
    "    b_scan_rgb = (b_scan_rgb / b_scan_rgb.max() * 255).astype(np.uint8)\n",
    "    img = Image.fromarray(b_scan_rgb)\n",
    "    b_scans.append(transform(img))\n",
    "b_scans = torch.stack(b_scans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88db71",
   "metadata": {},
   "source": [
    "## 3. Load Model and Extract Embeddings <a id = 'load'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c0d64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTModel(\n",
       "  (embeddings): ViTEmbeddings(\n",
       "    (patch_embeddings): ViTPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ViTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ViTLayer(\n",
       "        (attention): ViTSdpaAttention(\n",
       "          (attention): ViTSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (pooler): ViTPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Load Model\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a860f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Extract Embeddings\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for b_scan in b_scans:\n",
    "        inputs = b_scan.unsqueeze(0)\n",
    "        outputs = model(pixel_values=inputs)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "        embeddings.append(embedding.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b4fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a46c22",
   "metadata": {},
   "source": [
    "## 4. Save Embeddings <a id = 'save'></a>\n",
    "[Back to top](#table_of_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddab441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Save\n",
    "np.save('b_scan_embeddings.npy', embeddings)\n",
    "#np.save('volume_embedding.npy', volume_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7cc9a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ViTImageProcessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load model and preprocessor\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m processor = \u001b[43mViTImageProcessor\u001b[49m.from_pretrained(\u001b[33m'\u001b[39m\u001b[33mgoogle/vit-base-patch16-224\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m model = ViTModel.from_pretrained(\u001b[33m'\u001b[39m\u001b[33mgoogle/vit-base-patch16-224\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m model.eval()\n",
      "\u001b[31mNameError\u001b[39m: name 'ViTImageProcessor' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load model and preprocessor\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=Image.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# List of .mat files\n",
    "mat_files = [f'path_to_data/patient_{i:03d}.mat' for i in range(1, 201)]  # 200 files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21493fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "for mat_file in mat_files:\n",
    "    # Extract patient ID from filename (e.g., 'patient_001.mat' -> '001')\n",
    "    patient_id = mat_file.split('patient_')[1].split('.mat')[0]\n",
    "\n",
    "    # Load data\n",
    "    mat_data = sio.loadmat(mat_file)\n",
    "    image = mat_data['image']  # (512, 1000, 100)\n",
    "\n",
    "    # Process B-scans\n",
    "    b_scans = []\n",
    "    for i in range(image.shape[2]):\n",
    "        b_scan = image[:, :, i]\n",
    "        b_scan_rgb = np.stack([b_scan] * 3, axis=-1)\n",
    "        b_scan_rgb = (b_scan_rgb / b_scan_rgb.max() * 255).astype(np.uint8)\n",
    "        img = Image.fromarray(b_scan_rgb)\n",
    "        b_scans.append(transform(img))\n",
    "    b_scans = torch.stack(b_scans)\n",
    "\n",
    "    # Extract embeddings\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for b_scan in b_scans:\n",
    "            inputs = b_scan.unsqueeze(0)\n",
    "            outputs = model(pixel_values=inputs)\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "            embeddings.append(embedding.numpy())\n",
    "\n",
    "    embeddings = np.array(embeddings)  # (100, 768)\n",
    "    # Save embeddings\n",
    "    np.save(f'patient_{patient_id}_embeddings.npy', embeddings)\n",
    "    print(f\"Saved embeddings for patient {patient_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f401be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mapping_data = {'file_name': [f'patient_{i:03d}_embeddings.npy' for i in range(1, 201)],\n",
    "                'patient_id': [f'{i:03d}' for i in range(1, 201)]}\n",
    "df = pd.DataFrame(mapping_data)\n",
    "df.to_csv('embedding_map.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
